{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "526cb30f-b1cf-434c-9246-22cae57711e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e104b8f-2ea7-4eae-8e38-211a2e699e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom activation function\n",
    "def custom_activation(output):\n",
    "    logexpsum = backend.sum(backend.exp(output), axis=-1, keepdims=True)\n",
    "    result = logexpsum / (logexpsum + 1.0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "81bf8a2f-eb97-4e1c-a5e4-025348d146fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone supervised and unsupervised discriminator models\n",
    "def define_discriminator(in_shape=(96, 96, 3), n_classes=10, dropout_rate=0.2):\n",
    "    # Image input\n",
    "    in_image = layers.Input(shape=in_shape)\n",
    "    # Downsample\n",
    "    fe = layers.Conv2D(32, (3,3), activation=\"relu\")(in_image)\n",
    "    fe = layers.BatchNormalization()(fe)\n",
    "    fe = layers.MaxPooling2D((2, 2))(fe)\n",
    "    fe = layers.Dropout(rate=dropout_rate)(fe)\n",
    "    # Downsample\n",
    "    fe = layers.Conv2D(64, (3,3), activation=\"relu\")(fe)\n",
    "    fe = layers.BatchNormalization()(fe)\n",
    "    fe = layers.MaxPooling2D((2, 2))(fe)\n",
    "    fe = layers.Dropout(rate=dropout_rate)(fe)\n",
    "    # Downsample\n",
    "    fe = layers.Conv2D(128, (3,3), activation=\"relu\")(fe)\n",
    "    fe = layers.BatchNormalization()(fe)\n",
    "    fe = layers.MaxPooling2D((2, 2))(fe)\n",
    "    fe = layers.Dropout(rate=dropout_rate)(fe)\n",
    "    # Downsample\n",
    "    fe = layers.Conv2D(256, (3,3), activation=\"relu\")(fe)\n",
    "    fe = layers.BatchNormalization()(fe)\n",
    "    fe = layers.MaxPooling2D((2, 2))(fe)\n",
    "    fe = layers.Dropout(rate=dropout_rate)(fe)\n",
    "    # Downsample\n",
    "    fe = layers.Conv2D(512, (3,3), activation=\"relu\")(fe)\n",
    "    fe = layers.BatchNormalization()(fe)\n",
    "    fe = layers.MaxPooling2D((2, 2))(fe)\n",
    "    fe = layers.Dropout(rate=dropout_rate)(fe)\n",
    "    # Flatten feature maps\n",
    "    fe = layers.Flatten()(fe)\n",
    "    # Dense\n",
    "    fe = layers.Dense(64, activation=\"relu\")(fe)\n",
    "    fe = layers.BatchNormalization()(fe)\n",
    "    fe = layers.Dropout(rate=dropout_rate)(fe)\n",
    "    # Output layer nodes\n",
    "    fe = layers.Dense(n_classes)(fe)\n",
    "    # supervised output\n",
    "    c_out_layer = layers.Activation('softmax')(fe)\n",
    "    # define and compile supervised discriminator model\n",
    "    c_model = Model(in_image, c_out_layer)\n",
    "    c_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5), metrics=['accuracy'])\n",
    "    \n",
    "    # unsupervised output\n",
    "    d_out_layer = layers.Lambda(custom_activation)(fe)\n",
    "    # define and compile unsupervised discriminator model\n",
    "    d_model = Model(in_image, d_out_layer)\n",
    "    d_model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0002, beta_1=0.5))\n",
    "    \n",
    "    return d_model, c_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "835125ab-2a7f-4196-86db-11b0cd3c7d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim):\n",
    "    # Image generator input\n",
    "    in_lat = layers.Input(shape=(latent_dim,))\n",
    "    # Foundation for 6x6 image\n",
    "    n_nodes = 128 * 6 * 6\n",
    "    gen = layers.Dense(n_nodes)(in_lat)\n",
    "    gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = layers.Reshape((6, 6, 128))(gen)\n",
    "    # upsample to 12x12\n",
    "    gen = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding=\"same\")(gen)\n",
    "    gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
    "    # upsample to 24x24\n",
    "    gen = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding=\"same\")(gen)\n",
    "    gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
    "    # upsample to 48x48\n",
    "    gen = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding=\"same\")(gen)\n",
    "    gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
    "    # upsample to 96x96\n",
    "    gen = layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding=\"same\")(gen)\n",
    "    gen = layers.LeakyReLU(alpha=0.2)(gen)\n",
    "    # output\n",
    "    out_layer = layers.Conv2D(3, (7,7), activation='tanh', padding=\"same\")(gen)\n",
    "    # define model\n",
    "    model = Model(in_lat, out_layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9cdad217-02a3-48df-a34e-9cb57e463a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # connect image output from generator as input to discriminator\n",
    "    gan_output = d_model(g_model.output)\n",
    "    # define gan model as taking noise and outputting a classification\n",
    "    model = Model(g_model.input, gan_output)\n",
    "    # compile model\n",
    "    opt = Adam(learning_rate=0.0002, beta_1=0.5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91b8f447-deac-4651-b466-e8542fdbb374",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    # Train set\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    i = 0\n",
    "    for folder in glob.iglob(\"./dataset/train/*\"):\n",
    "        for file in glob.iglob(f\"{folder}/*\"):\n",
    "            train_images.append(np.asarray(Image.open(file)))\n",
    "            train_labels.append(np.asarray(i))\n",
    "        i += 1\n",
    "    \n",
    "    # Test set\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    i = 0\n",
    "    for folder in glob.iglob(\"./dataset/test/*\"):\n",
    "        for file in glob.iglob(f\"{folder}/*\"):\n",
    "            test_images.append(np.asarray(Image.open(file)))\n",
    "            test_labels.append(np.asarray(i))\n",
    "        i += 1\n",
    "        \n",
    "    train_images = np.asarray(train_images)\n",
    "    train_labels = to_categorical(train_labels, 10)\n",
    "    \n",
    "    test_images = np.asarray(test_images)\n",
    "    test_labels = to_categorical(test_labels, 10)\n",
    "    \n",
    "    return train_images, test_images, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c766c442-4c55-4fa7-a1ef-a2224454f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_unlabeled_dataset():\n",
    "    images = []\n",
    "    # Unlabeled data\n",
    "    for f in glob.iglob(\"./dataset/unlabeled/*\"):\n",
    "        images.append(np.asarray(Image.open(f)))\n",
    "        \n",
    "    images = np.array(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d1b35298-d12a-4eba-8c25-d9951621afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_images(img):\n",
    "    img_normalized = cv.normalize(img, None, 0, 1.0, cv.NORM_MINMAX, dtype=cv.CV_32F)\n",
    "    return img_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7be67dc2-fbb0-4437-b348-e1b813d69feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix = np.random.randint(0, images.shape[0], n_samples)\n",
    "    # select images and labels\n",
    "    x, = images[ix], \n",
    "    if labels is not None:\n",
    "        labels = labels[ix]\n",
    "    # generate class labels\n",
    "    y = np.ones((n_samples, 1))\n",
    "    return x, labels, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0aa6b2a1-4da4-41cf-90a8-bfb6f4980244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # generate points in the latent space\n",
    "    z_input = np.random.randn(latent_dim * n_samples)\n",
    "    # reshape into a batch of inputs for the network\n",
    "    z_input = z_input.reshape(n_samples, latent_dim)\n",
    "    return z_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "48fd2823-cfd6-4beb-9ffc-a74f717e98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # generate points in latent space\n",
    "    z_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    images = generator.predict(z_input)\n",
    "    # create class labels\n",
    "    y = np.zeros((n_samples, 1))\n",
    "    return images, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b165dcc9-3025-4e6d-b250-f3d9bc111d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, c_model, gan_model, x_train, y_train, x_test, y_test, unlabeled_data, latent_dim, n_epochs=10, n_batch=64):\n",
    "    # calculate the number of batches per training epoch\n",
    "    bat_per_epo = int(unlabeled_data.shape[0] / n_batch)\n",
    "    # calculate the size of half a batch of samples\n",
    "    half_batch = int(n_batch / 2)\n",
    "    #print('n_epochs=%d, n_batch=%d, 1/2=%d, b/e=%d, steps=%d' % (n_epochs, n_batch, half_batch, bat_per_epo, n_steps))\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_epochs):\n",
    "        for j in range(bat_per_epo):\n",
    "            # Start timer\n",
    "            start_t = time.time()\n",
    "            \n",
    "            # update supervised discriminator (c)\n",
    "            x_batch, y_batch, _ = generate_real_samples((x_train, y_train), n_batch)\n",
    "            c_loss, c_acc = c_model.train_on_batch(x_batch, y_batch)\n",
    "            \n",
    "            # update unsupervised discriminator (d)\n",
    "            x_real, _, y_real = generate_real_samples((unlabeled_data, None), half_batch)\n",
    "            d_loss1 = d_model.train_on_batch(x_real, y_real)\n",
    "            x_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            d_loss2 = d_model.train_on_batch(x_fake, y_fake)\n",
    "            \n",
    "            # update generator (g)\n",
    "            x_gan, y_gan = generate_latent_points(latent_dim, n_batch), np.ones((n_batch, 1))\n",
    "            g_loss = gan_model.train_on_batch(x_gan, y_gan)\n",
    "            \n",
    "            # End timer\n",
    "            tpb = time.time() - start_t\n",
    "            print(f\"batch {j} / {bat_per_epo}: tpb - {tpb:.2f}\" ,end='\\r')\n",
    "        \n",
    "        # evaluate the model performance every so often\n",
    "        c_loss, c_acc = c_model.evaluate(x_train, y_train)\n",
    "        c_loss_test, c_acc_test = c_model.evaluate(x_test, y_test)\n",
    "        print(f\">{i+1}\")\n",
    "        c_model.save(f\"./model_weights/GAN/run3/model_{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "630d01ca-31da-45d8-b7d7-f6fa4880d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images\n",
    "x_train, x_test, y_train, y_test = load_dataset()\n",
    "unlabeled_data = load_unlabeled_dataset()\n",
    "# Normalize data\n",
    "x_train = normalize_images(x_train)\n",
    "x_test = normalize_images(x_test)\n",
    "unlabeled_data = normalize_images(unlabeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e112ed1f-4444-4eea-8fdf-f6094c0d3123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size of the latent space\n",
    "latent_dim = 512\n",
    "# Create the discriminator models\n",
    "d_model, c_model = define_discriminator(dropout_rate=0.35)\n",
    "# Create the generator\n",
    "g_model = define_generator(latent_dim)\n",
    "# Create the gan\n",
    "gan_model = define_gan(g_model, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f8c856-1c2f-4e9b-a11e-ec3b91c387b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 4.8686 - accuracy: 0.1395\n",
      "94/94 [==============================] - 1s 8ms/step - loss: 4.8771 - accuracy: 0.1387\n",
      ">1\n",
      "INFO:tensorflow:Assets written to: ./model_weights/GAN/model_0\\assets\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 3.1411 - accuracy: 0.2574\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 3.2142 - accuracy: 0.2517\n",
      ">2\n",
      "INFO:tensorflow:Assets written to: ./model_weights/GAN/model_1\\assets\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.8469 - accuracy: 0.3343\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 3.0538 - accuracy: 0.3213\n",
      ">3\n",
      "INFO:tensorflow:Assets written to: ./model_weights/GAN/model_2\\assets\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.2830 - accuracy: 0.4459\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 2.6162 - accuracy: 0.4153\n",
      ">4\n",
      "INFO:tensorflow:Assets written to: ./model_weights/GAN/model_3\\assets\n",
      "313/313 [==============================] - 2s 7ms/step - loss: 2.7420 - accuracy: 0.4149\n",
      "94/94 [==============================] - 1s 7ms/step - loss: 3.2408 - accuracy: 0.3770\n",
      ">5\n",
      "INFO:tensorflow:Assets written to: ./model_weights/GAN/model_4\\assets\n",
      "batch 724 / 1562: tpb - 0.36\r"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train(g_model, d_model, c_model, gan_model, x_train, y_train, x_test, y_test, unlabeled_data, latent_dim, n_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ffde2936-1aaf-45b3-9d1e-525994509525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 12s 132ms/step - loss: 2.8632 - accuracy: 0.4623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8632307052612305, 0.4623333215713501]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac367da-2685-44b1-918f-a801931c6dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
